{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgbwHH6sEOQ0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import rotate\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalMaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate, add\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.metrics import Recall, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67KRmTt1EOz9"
   },
   "outputs": [],
   "source": [
    "# images attributes \n",
    "img_size = 256 \n",
    "\n",
    "# training attributes\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "# data paths\n",
    "project_path = \"./\"\n",
    "orto_path = project_path + \"data/geoportal_orto/\"\n",
    "masks_path = project_path + \"data/geoportal_build_mask/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "0tkgnKSkEO8i",
    "outputId": "c64fac41-b389-4259-ac04-55f900efea5b"
   },
   "outputs": [],
   "source": [
    "# list of names all images in the given path\n",
    "ids = sorted(os.listdir(orto_path))\n",
    "test_ids = sorted(os.listdir(test_orto_path))\n",
    "\n",
    "# split train and valid\n",
    "ids_train, ids_valid = train_test_split(ids, test_size=0.2, random_state=123)\n",
    "\n",
    "print(\"Total images = \", len(ids))\n",
    "print(\"Train images = \", len(ids_train))\n",
    "print(\"Valid images = \", len(ids_valid))\n",
    "print(\"Test images = \", len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EotFdMaoEO_b"
   },
   "outputs": [],
   "source": [
    "def rotate_image(image, angle=180):\n",
    "    return rotate(image, angle, resize=True, preserve_range=True)\n",
    "\n",
    "\n",
    "def data_gen(img_folder, mask_folder, img_ids, img_size, batch_size, random_rotate=True):\n",
    "    c = 0\n",
    "    random.shuffle(img_ids)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        img = np.zeros((batch_size, img_size, img_size, 3)).astype(float)\n",
    "        mask = np.zeros((batch_size, img_size, img_size, 1)).astype(float)\n",
    "\n",
    "        for i in range(c, c+batch_size): # initially from 0 to 16, c = 0. \n",
    "\n",
    "            # load image    \n",
    "            img_data = img_to_array(load_img(img_folder+img_ids[i]))/255.0\n",
    "            \n",
    "            \n",
    "            # load mask\n",
    "            img_mask = img_to_array(load_img(mask_folder+img_ids[i], color_mode=\"grayscale\"))\n",
    "            img_mask = (img_mask > img_mask.min()).astype(int)\n",
    "            # add extra dimension for parity with train_img size [img_size * img_size * 3]\n",
    "            img_mask = img_mask.reshape(img_size, img_size, 1)\n",
    "            \n",
    "            if random_rotate:\n",
    "                rotate_angle = random.choice((0, 90, 180, 270))\n",
    "                img_data = rotate_image(img_data, rotate_angle)\n",
    "                img_data = rotate_image(img_data, rotate_angle)             \n",
    "            \n",
    "            # add to array - img[0], img[1], and so on.\n",
    "            img[i-c] = img_data \n",
    "            mask[i-c] = img_mask\n",
    "\n",
    "        c += batch_size\n",
    "        if c+batch_size >= len(img_ids):\n",
    "            c=0\n",
    "            random.shuffle(img_ids)\n",
    "\n",
    "        yield img, mask\n",
    "\n",
    "# preparing data generators\n",
    "train_gen = data_gen(orto_path, masks_path, ids_train, img_size, batch_size, random_rotate=True)\n",
    "valid_gen = data_gen(orto_path, masks_path, ids_valid, img_size, batch_size, random_rotate=False)\n",
    "test_gen = data_gen(test_orto_path, test_masks_path, test_ids, img_size, batch_size, random_rotate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4sIkvjHEPB6"
   },
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sJPRLBiF92V"
   },
   "outputs": [],
   "source": [
    "def get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDfSurDzIEHZ"
   },
   "outputs": [],
   "source": [
    "input_img = Input((img_size, img_size, 3), name='img')\n",
    "model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=False)\n",
    "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\", Recall(), Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5OXKIDy6IEap",
    "outputId": "fb090e58-03d2-4c34-a7b5-ad9297ec0823"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "z_wWdHtP9XLv",
    "outputId": "12abc46a-7528-4c74-8fd8-072e064d035f"
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "checkpoints_path = project_path + 'checkpoints/'\n",
    "\n",
    "if not os.path.exists(checkpoints_path): \n",
    "        os.makedirs(checkpoints_path)\n",
    "\n",
    "model_name = f\"unet_ep_{epochs}_bs_{batch_size}_{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gG-h8WPmIEdB"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(checkpoints_path + model_name + \".h5\", verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "M7PzFjz8INTH",
    "outputId": "d8f8c8b3-ae13-46a8-9708-e4129e7bb776"
   },
   "outputs": [],
   "source": [
    "results = model.fit(\n",
    "    train_gen, \n",
    "    steps_per_epoch=len(ids_train)//batch_size,\n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=len(ids_valid)//batch_size, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSNF9xieINgL"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8)) \n",
    "plt.title(\"Learning curve\") \n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\") \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_T2_iokds4P"
   },
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model.load_weights(checkpoints_path + model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PLcqf0LpLLE"
   },
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    test_gen, \n",
    "    steps=len(test_ids)//batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9Oa4DE_INjw"
   },
   "outputs": [],
   "source": [
    "def plot_sample(img_folder, mask_folder, img_ids, thresh=0.5, i=None):\n",
    "    \"\"\"Function to plot the results\"\"\"\n",
    "    if i is None:\n",
    "        i = random.randint(0, len(img_ids))\n",
    "    \n",
    "    img = np.zeros((1, img_size, img_size, 3)).astype(float)\n",
    "    mask = np.zeros((1, img_size, img_size, 1)).astype(float)\n",
    "    \n",
    "    # load image    \n",
    "    img_data = img_to_array(load_img(img_folder+img_ids[i]))/255.0\n",
    "\n",
    "    # load mask\n",
    "    img_mask = img_to_array(load_img(mask_folder+img_ids[i], color_mode=\"grayscale\"))\n",
    "    img_mask = (img_mask > img_mask.min()).astype(int)\n",
    "    # add extra dimension for parity with train_img size [img_size * img_size * 3]\n",
    "    img_mask = img_mask.reshape(img_size, img_size, 1)             \n",
    "\n",
    "    # add to array\n",
    "    img[0] = img_data \n",
    "    mask[0] = img_mask\n",
    "    \n",
    "    # print file id for reference\n",
    "    print(img_ids[i])\n",
    "\n",
    "    # mask prediction\n",
    "    mask_pred = model.predict(img)[0]\n",
    "    mask_pred_t = (mask_pred > thresh).astype(np.uint8)\n",
    "\n",
    "    # mask diff\n",
    "    mask_diff = mask_pred_t * (img_mask<1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(30, 10))\n",
    "    ax[0].imshow(img[0])\n",
    "    ax[0].set_title('orto rgb')\n",
    "\n",
    "    ax[1].imshow(mask.squeeze())\n",
    "    ax[1].set_title('mask')\n",
    "\n",
    "    ax[2].imshow(mask_pred.squeeze())\n",
    "    ax[2].set_title('mask predicted')\n",
    "\n",
    "    ax[3].imshow(mask_pred_t.squeeze())\n",
    "    ax[3].set_title('mask predicted binary')\n",
    "    \n",
    "    ax[4].imshow(mask_diff.squeeze())\n",
    "    ax[4].set_title('mask predicted binary - mask');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkBr1xJrIEf6"
   },
   "outputs": [],
   "source": [
    "plot_sample(test_orto_path, test_masks_path, img_ids=test_ids, thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXDHy-1iIEiN"
   },
   "outputs": [],
   "source": [
    "plot_sample(test_orto_path, test_masks_path, img_ids=test_ids, thresh=0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
